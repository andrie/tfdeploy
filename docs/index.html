<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>tfdeploy: Deploy TensorFlow Models • tfdeploy</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="jquery.sticky-kit.min.js"></script><script src="pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">tfdeploy</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="articles/saved-models-from-R.html">Using Saved Models from R</a>
</li>
<li>
  <a href="reference/index.html">Reference</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/rstudio/tfdeploy">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    

    
    
<div class="contents">
<div id="tensorflow-model-deployment-from-r" class="section level1">
<div class="page-header"><h1 class="hasAnchor">
<a href="#tensorflow-model-deployment-from-r" class="anchor"></a>TensorFlow Model Deployment from R</h1></div>
<div id="overview" class="section level2">
<h2 class="hasAnchor">
<a href="#overview" class="anchor"></a>Overview</h2>
<p>While TensorFlow models are typically defined and trained using R or Python code, it is possible to deploy TensorFlow models in a wide variety of environments without any runtime dependency on R or Python:</p>
<ul>
<li><p><a href="https://www.tensorflow.org/serving/">TensorFlow Serving</a> is an open-source software library for serving TensorFlow models using a <a href="https://grpc.io/">gRPC</a> interface.</p></li>
<li><p><a href="https://tensorflow.rstudio.com/tools/cloudml/">CloudML</a> is a managed cloud service that serves TensorFlow models using a <a href="https://cloud.google.com/ml-engine/reference/rest/v1/projects/predict">REST</a> interface.</p></li>
<li><p><a href="https://www.rstudio.com/products/connect/">RStudio Connect</a> provides support for serving models using the same REST API as CloudML, but on a server within your own organization.</p></li>
</ul>
<p>TensorFlow models can also be deployed to <a href="https://www.tensorflow.org/mobile/tflite/">mobile</a> and <a href="https://aws.amazon.com/blogs/machine-learning/how-to-deploy-deep-learning-models-with-aws-lambda-and-tensorflow/">embedded</a> devices including iOS and Android mobile phones and Raspberry Pi computers.</p>
<p>The R interface to TensorFlow includes a variety of tools designed to make exporting and serving TensorFlow models straightforward. The basic process for deploying TensorFlow models from R is as follows:</p>
<ul>
<li><p>Train a model using the <a href="https://tensorflow.rstudio.com/keras/">keras</a>, <a href="https://tensorflow.rstudio.com/tfestimators/">tfestimators</a>, or <a href="https://tensorflow.rstudio.com/tensorflow/">tensorflow</a> R packages.</p></li>
<li><p>Call the <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel()</a></code> function on your trained model to write it to disk as a TensorFlow SavedModel.</p></li>
<li><p>Use the <code><a href="reference/serve_savedmodel.html">serve_savedmodel()</a></code> from the <a href="https://tensorflow.rstudio.com/tools/tfdeploy/">tfdeploy</a> package to run a local test server that supports the same REST API as CloudML and RStudio Connect.</p></li>
<li><p>Deploy your model using TensorFlow Serving, CloudML, or RStudio Connect.</p></li>
</ul>
</div>
<div id="getting-started" class="section level2">
<h2 class="hasAnchor">
<a href="#getting-started" class="anchor"></a>Getting Started</h2>
<p>Begin by installing the <strong>tfdeploy</strong> package from GitHub as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/install_github">install_github</a></span>(<span class="st">"rstudio/tfdeploy"</span>)</code></pre></div>
<p>First we’ll walk through an end-to-end example that: trains a Keras model with the MNIST dataset, exports the saved model, and then serves the exported model locally for predictions with a REST API. After that we’ll describe in more depth the specific requirements and various options associated with exporting models. Finally, we’ll cover the various deployment options and provide links to additional documentation.</p>
<div id="end-to-end-example" class="section level3">
<h3 class="hasAnchor">
<a href="#end-to-end-example" class="anchor"></a>End-to-End Example</h3>
<p>We’ll use a Keras model that recognizes handwritten digits from the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset as an example. MNIST consists of 28 x 28 grayscale images of handwritten digits like these:</p>
<p><img style="width: 50%;" src="images/MNIST.png"></p>
<p>The dataset also includes labels for each image. For example, the labels for the above images are 5, 0, 4, and 1.</p>
<p>Here’s the complete source code for the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(keras)

<span class="co"># load data</span>
<span class="kw">c</span>(<span class="kw">c</span>(x_train, y_train), <span class="kw">c</span>(x_test, y_test)) <span class="op">%&lt;-%</span><span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/dataset_mnist">dataset_mnist</a></span>()

<span class="co"># reshape and rescale</span>
x_train &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">array_reshape</a></span>(x_train, <span class="dt">dim =</span> <span class="kw">c</span>(<span class="kw">nrow</span>(x_train), <span class="dv">784</span>)) <span class="op">/</span><span class="st"> </span><span class="dv">255</span>
x_test &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/reexports">array_reshape</a></span>(x_test, <span class="dt">dim =</span> <span class="kw">c</span>(<span class="kw">nrow</span>(x_test), <span class="dv">784</span>)) <span class="op">/</span><span class="st"> </span><span class="dv">255</span>

<span class="co"># one-hot encode response</span>
y_train &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/to_categorical">to_categorical</a></span>(y_train, <span class="dv">10</span>)
y_test &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/to_categorical">to_categorical</a></span>(y_test, <span class="dv">10</span>)

<span class="co"># define and compile model</span>
model &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/keras_model_sequential">keras_model_sequential</a></span>()
model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">256</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">784</span>),
              <span class="dt">name =</span> <span class="st">"image"</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">128</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">'softmax'</span>,
              <span class="dt">name =</span> <span class="st">"prediction"</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/compile">compile</a></span>(
    <span class="dt">loss =</span> <span class="st">'categorical_crossentropy'</span>,
    <span class="dt">optimizer =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/optimizer_rmsprop">optimizer_rmsprop</a></span>(),
    <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">'accuracy'</span>)
  )

<span class="co"># train model</span>
history &lt;-<span class="st"> </span>model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/fit">fit</a></span>(
  x_train, y_train,
  <span class="dt">epochs =</span> <span class="dv">35</span>, <span class="dt">batch_size =</span> <span class="dv">128</span>,
  <span class="dt">validation_split =</span> <span class="fl">0.2</span>
)</code></pre></div>
<p>In R, it is easy to make predictions using the the trained model and R’s <code>predict</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">preds &lt;-<span class="st"> </span><span class="kw">predict</span>(model, x_test[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,])</code></pre></div>
<pre><code>        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    0    0    0    0    0    0    0    1    0     0
[2,]    0    0    1    0    0    0    0    0    0     0
[3,]    0    1    0    0    0    0    0    0    0     0
[4,]    1    0    0    0    0    0    0    0    0     0</code></pre>
<p>Each row represents an image, each column represents a digit from 0-9, and the values represent the model’s prediction. For example, the first image is predicted to be a 7.</p>
<p>What if we want to deploy the model in an environment where R isn’t available? The next sections cover exporting and deploying the model with the <strong>tfdeploy</strong> package.</p>
<div id="exporting-the-model" class="section level4">
<h4 class="hasAnchor">
<a href="#exporting-the-model" class="anchor"></a>Exporting the Model</h4>
<p>After training, the next step is to export the model as a TensorFlow SavedModel using the <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel()</a></code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tfdeploy)
<span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel</a></span>(model, <span class="st">"savedmodel"</span>)</code></pre></div>
<p>This will create a “savedmodel” directory that contains a saved version of your MNIST model. You can view the graph of your model using TensorBoard with the <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/view_savedmodel">view_savedmodel()</a></code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/view_savedmodel">view_savedmodel</a></span>(<span class="st">"savedmodel"</span>)</code></pre></div>
</div>
<div id="using-the-exported-model" class="section level4">
<h4 class="hasAnchor">
<a href="#using-the-exported-model" class="anchor"></a>Using the Exported Model</h4>
<p>To test the exported model locally, use the <code>serve_savedmodel</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="reference/serve_savedmodel.html">serve_savedmodel</a></span>(<span class="st">'savedmodel'</span>, <span class="dt">browse =</span> <span class="ot">TRUE</span>)</code></pre></div>
<div class="figure">
<img src="images/swagger.png" class="illustration" style="width:80.0%">
</div>
<p>The model is served at <a href="http://localhost:8989" class="uri">http://localhost:8989</a> along with a webpage that describes the REST interface to the model. The REST interface is based on the <a href="https://cloud.google.com/ml-engine/docs/v1/predict-request">CloudML predict request API</a>.</p>
<p>The model can be used for prediction by making HTTP POST requests. The body of the request should contain the new instances of data and the HTTP response provides the model’s predictions. <strong>The data in the request body should be pre-processed and formatted in the same way as the original training data</strong>. For MNIST, the request body could be a JSON file containing one or more pre-processed images:</p>
<pre class="text"><code>{
  "instances": [
    {
      "image_input": [0.12,0,0.79,...,0,0]
    }
  ]
}</code></pre>
<p>The HTTP POST request would be:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">curl</span> -X POST -H <span class="st">"Content-Type: application/json"</span> -d @new_image.json http://localhost:8089/serving_default/predict</code></pre></div>
<p>Similar to R’s predict function, the response includes an array representing the digits 0-9. The image in <code>new_image.json</code> is predicted to be a 7:</p>
<pre><code>{"predictions":[{"prediction":[1.3306e-24,4.9968e-26,1.8917e-23,1.7047e-21,0,8.963e-33,0,1,2.3306e-32,2.0314e-22]}]}</code></pre>
</div>
<div id="deploying-the-model" class="section level4">
<h4 class="hasAnchor">
<a href="#deploying-the-model" class="anchor"></a>Deploying the Model</h4>
<p>Once you are satisifed with local testing, the next step is to make the model available to others. There are a number of available options for this including <a href="#tensorflow-serving">TensorFlow Serving</a>, <a href="#cloudml">CloudML</a>, and <a href="#rstudio-connect">RStudio Connect</a>. For example, to deploy the saved model to CloudML we could use the cloudml package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(cloudml)
<span class="kw"><a href="http://www.rdocumentation.org/packages/cloudml/topics/cloudml_deploy">cloudml_deploy</a></span>(<span class="st">"savedmodel"</span>, <span class="dt">name =</span> <span class="st">"keras_mnist"</span>, <span class="dt">version =</span> <span class="st">"keras_mnist_1"</span>)</code></pre></div>
<p>The same HTTP POST request we used to test the model locally can be used to generate predictions on CloudML, provided the proper access to the CloudML API.</p>
<p>Now that we’ve deployed a simple end-to-end example, we’ll describe the process of <a href="#model-export">Model Export</a> and <a href="#model-deployment">Model Deployment</a> in more detail.</p>
</div>
</div>
</div>
<div id="model-export" class="section level2">
<h2 class="hasAnchor">
<a href="#model-export" class="anchor"></a>Model Export</h2>
<p>TensorFlow SavedModel defines a language-neutral format to save machine-learned models that is recoverable and hermetic. It enables higher-level systems and tools to produce, consume and transform TensorFlow models.</p>
<p>The <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel()</a></code> function creates a SavedModel from a model trained using the keras, tfestimators, or tensorflow R packages. There are subtle differences in how this works in practice depending on the package you are using.</p>
<div id="keras" class="section level3">
<h3 class="hasAnchor">
<a href="#keras" class="anchor"></a>keras</h3>
<p>The <a href="End-to-End%20Example">Keras Example</a> above includes complete example code for creating and using SavedModel instances from Keras so we won’t repeat all of those details here.</p>
<p>To export a TensorFlow SavedModel from a Keras model, simply call the <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel()</a></code> function on any Keras model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel</a></span>(model, <span class="st">"savedmodel"</span>)</code></pre></div>
<pre style="background-color: transparent; margin: 0 !important; padding: 0 !important;"><code style="color: rgb(196,27,6); background-color: transparent;">Keras learning phase set to 0 for export (restart R session before doing additional training)

</code></pre>
<p>Note the printed output. Exporting a Keras model requires setting the Keras “learning phase” to 0. In practice, this means that after calling <code>export_savedmodel</code> <strong>you can not continue to train the model in the same R session</strong>.</p>
<p>It is important to assign reasonable names to the the first and last layers. For example, in the model code above we named the first layer “image” and the last layer “prediction”.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">256</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>, <span class="dt">input_shape =</span> <span class="kw">c</span>(<span class="dv">784</span>),
              <span class="dt">name =</span> <span class="st">"image"</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">128</span>, <span class="dt">activation =</span> <span class="st">'relu'</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="http://www.rdocumentation.org/packages/keras/topics/layer_dense">layer_dense</a></span>(<span class="dt">units =</span> <span class="dv">10</span>, <span class="dt">activation =</span> <span class="st">'softmax'</span>,
              <span class="dt">name =</span> <span class="st">"prediction"</span>)</code></pre></div>
<p>The layer names are reflected in the structure of requests and responses to and from the deployed model.</p>
</div>
<div id="tfestimators" class="section level3">
<h3 class="hasAnchor">
<a href="#tfestimators" class="anchor"></a>tfestimators</h3>
<p>Exporting a TensorFlow SavedModel from a TF Estimators model works exactly the same way as exporting a keras model, simply call <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel()</a></code> on the estimator. Here is a complete example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tfestimators)

mtcars_input_fn &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">num_epochs =</span> <span class="dv">1</span>) {
  <span class="kw"><a href="http://www.rdocumentation.org/packages/tfestimators/topics/input_fn">input_fn</a></span>(data,
           <span class="dt">features =</span> <span class="kw">c</span>(<span class="st">"disp"</span>, <span class="st">"cyl"</span>),
           <span class="dt">response =</span> <span class="st">"mpg"</span>,
           <span class="dt">batch_size =</span> <span class="dv">32</span>,
           <span class="dt">num_epochs =</span> num_epochs)
}

cols &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tfestimators/topics/feature_columns">feature_columns</a></span>(<span class="kw"><a href="http://www.rdocumentation.org/packages/tfestimators/topics/column_numeric">column_numeric</a></span>(<span class="st">"disp"</span>), <span class="kw"><a href="http://www.rdocumentation.org/packages/tfestimators/topics/column_numeric">column_numeric</a></span>(<span class="st">"cyl"</span>))

model &lt;-<span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tfestimators/topics/linear_estimators">linear_regressor</a></span>(<span class="dt">feature_columns =</span> cols)

indices &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(mtcars), <span class="dt">size =</span> <span class="fl">0.80</span> <span class="op">*</span><span class="st"> </span><span class="kw">nrow</span>(mtcars))
train &lt;-<span class="st"> </span>mtcars[indices, ]
test  &lt;-<span class="st"> </span>mtcars[<span class="op">-</span>indices, ]

model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw"><a href="http://www.rdocumentation.org/packages/tfestimators/topics/reexports">train</a></span>(<span class="kw">mtcars_input_fn</span>(train, <span class="dt">num_epochs =</span> <span class="dv">10</span>))

<span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel</a></span>(model, <span class="st">"savedmodel"</span>)</code></pre></div>
<p>Generating predictions is done in the same way as with exported Keras models. First, use <code>serve_savedmodel</code> to host the model locally. Once running, an HTTP POST request can be made:</p>
<pre><code>curl -X POST "http://127.0.0.1:8089/predict/predict/" -H "accept: application/json" -H "Content-Type: application/json" -d "{ \"instances\": [ { \"disp\": [ 160 ], \"cyl\": [ 4 ] } ]}"</code></pre>
<p>Each instance of new data should be formatted as a json array, and each element in the array should be a named array corresponding to the feature columns. This structure is similar to a named list in R.</p>
<p>The response is the predicted MPG:</p>
<pre><code>{
  "predictions": [
    {
      "predictions": [
        8.4974
      ]
    }
  ]
}</code></pre>
</div>
<div id="tensorflow" class="section level3">
<h3 class="hasAnchor">
<a href="#tensorflow" class="anchor"></a>tensorflow</h3>
<p>The <a href="https://tensorflow.rstudio.com/tensorflow">tensorflow</a> package provides a lower-level interface to the TensorFlow API. You can also use the <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel()</a></code> function to export models created with this API, however you need to provide some additional parmaeters indicating which tensors represent the inputs and outputs for your model.</p>
<p>For example, here’s an MNIST model using the core TensorFlow API along with the requisite call to <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel()</a></code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tensorflow)

sess &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Session</span>()
datasets &lt;-<span class="st"> </span>tf<span class="op">$</span>contrib<span class="op">$</span>learn<span class="op">$</span>datasets
mnist &lt;-<span class="st"> </span>datasets<span class="op">$</span>mnist<span class="op">$</span><span class="kw">read_data_sets</span>(<span class="st">"MNIST-data"</span>, <span class="dt">one_hot =</span> <span class="ot">TRUE</span>)

<span class="co"># Note that we define x as the input tensor</span>
<span class="co"># and y as the output tensor that will contain</span>
<span class="co"># the scores. These are referenced in export_savedmodel</span>
x &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">placeholder</span>(tf<span class="op">$</span>float32, <span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/shape">shape</a></span>(<span class="ot">NULL</span>, 784L))
W &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Variable</span>(tf<span class="op">$</span><span class="kw">zeros</span>(<span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/shape">shape</a></span>(784L, 10L)))
b &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">Variable</span>(tf<span class="op">$</span><span class="kw">zeros</span>(<span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/shape">shape</a></span>(10L)))
y &lt;-<span class="st"> </span>tf<span class="op">$</span>nn<span class="op">$</span><span class="kw">softmax</span>(tf<span class="op">$</span><span class="kw">matmul</span>(x, W) <span class="op">+</span><span class="st"> </span>b)
y_ &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">placeholder</span>(tf<span class="op">$</span>float32, <span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/shape">shape</a></span>(<span class="ot">NULL</span>, 10L))
cross_entropy &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">reduce_mean</span>(
  <span class="op">-</span>tf<span class="op">$</span><span class="kw">reduce_sum</span>(y_ <span class="op">*</span><span class="st"> </span>tf<span class="op">$</span><span class="kw">log</span>(y), <span class="dt">reduction_indices=</span>1L)
)

optimizer &lt;-<span class="st"> </span>tf<span class="op">$</span>train<span class="op">$</span><span class="kw">GradientDescentOptimizer</span>(<span class="fl">0.5</span>)
train_step &lt;-<span class="st"> </span>optimizer<span class="op">$</span><span class="kw">minimize</span>(cross_entropy)

init &lt;-<span class="st"> </span>tf<span class="op">$</span><span class="kw">global_variables_initializer</span>()
sess<span class="op">$</span><span class="kw">run</span>(init)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>) {
  batches &lt;-<span class="st"> </span>mnist<span class="op">$</span>train<span class="op">$</span><span class="kw">next_batch</span>(100L)
  batch_xs &lt;-<span class="st"> </span>batches[[<span class="dv">1</span>]]
  batch_ys &lt;-<span class="st"> </span>batches[[<span class="dv">2</span>]]
  sess<span class="op">$</span><span class="kw">run</span>(train_step,
           <span class="dt">feed_dict =</span> <span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/reexports">dict</a></span>(<span class="dt">x =</span> batch_xs, <span class="dt">y_ =</span> batch_ys))
}

<span class="kw"><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel</a></span>(
  sess,
  <span class="st">"savedmodel"</span>,
  <span class="dt">inputs =</span> <span class="kw">list</span>(<span class="dt">image_input =</span> x),
  <span class="dt">outputs =</span> <span class="kw">list</span>(<span class="dt">scores =</span> y))</code></pre></div>
<p>Once the model is exported, the same process of using <code>serve_savedmodel</code> can be used, and the same HTTP requests demonstrated in the Keras example can be used against the tensorflow model.</p>
</div>
</div>
<div id="model-deployment" class="section level2">
<h2 class="hasAnchor">
<a href="#model-deployment" class="anchor"></a>Model Deployment</h2>
<p>There are a variety of ways to deploy a TensorFlow SavedModel, each of which are described below. Of the 3 methods described, 2 of them (CloudML and RStudio Connect) share the same REST interface that we have been using with <code>serve_savedmodel</code> to test locally. The REST interface is described in detail here: <a href="https://cloud.google.com/ml-engine/docs/v1/predict-request" class="uri">https://cloud.google.com/ml-engine/docs/v1/predict-request</a>.</p>
<div id="cloudml" class="section level3">
<h3 class="hasAnchor">
<a href="#cloudml" class="anchor"></a>CloudML</h3>
<p>You can deploy TensorFlow SavedModels to Google’s <a href="https://cloud.google.com/ml-engine/">CloudML</a> service using functions from the <a href="https://tensorflow.rstudio.com/tools/cloudml/">cloudml</a> package. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(cloudml)
<span class="kw"><a href="http://www.rdocumentation.org/packages/cloudml/topics/cloudml_deploy">cloudml_deploy</a></span>(<span class="st">"savedmodel"</span>, <span class="dt">name =</span> <span class="st">"keras_mnist"</span>)</code></pre></div>
<p>Once deployed to CloudML, predictions can be made using the same REST interace we previously used locally. The HTTP POST requests will be similar to the sample requests, but CloudML additiobnally requires proper authorization.</p>
<p>See the <a href="https://tensorflow.rstudio.com/tools/cloudml/articles/deployment.html">Deploying Models</a> article on the CloudML package website for additional details.</p>
</div>
<div id="rstudio-connect" class="section level3">
<h3 class="hasAnchor">
<a href="#rstudio-connect" class="anchor"></a>RStudio Connect</h3>
<p><a href="https://www.rstudio.com/products/connect/">RStudio Connect</a> is a publishing platform for applications, reports, and APIs created with R. Connect runs on-premise or in your own cloud infrastructure, giving you full control of the deployment environment. Connect can also integrate with your own security services and user management tools.</p>
<p>An upcoming version of RStudio Connect will include support for hosting TensorFlow SavedModels, using the same REST interface as is supported by the local server and CloudML.</p>
<p>Exported models will be published to Connect using the <code>rsconnect</code> package, for example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rsconnect<span class="op">::</span><span class="kw">deployTFModel</span>(<span class="st">'savedmodel'</span>, <span class="dt">account =</span> <span class="op">&lt;</span>username<span class="op">&gt;</span>, <span class="dt">server =</span> <span class="op">&lt;</span>internal_connect_server<span class="op">&gt;</span>)</code></pre></div>
<p>If you would like to preview the feature, or get more information, contact <a href="mailto:sales@rstudio.com">sales@rstudio.com</a>.</p>
</div>
<div id="tensorflow-serving" class="section level3">
<h3 class="hasAnchor">
<a href="#tensorflow-serving" class="anchor"></a>TensorFlow Serving</h3>
<p><a href="https://www.tensorflow.org/serving">TensorFlow Serving</a> is an open-source library and server implementation that allows you to serve TensorFlow SavedModels using a <a href="https://grpc.io/">gRPC interface</a> as opposed to the REST interface offered by the previous deployment tools.</p>
<p>Once you have exported a TensorFlow model using <code><a href="http://www.rdocumentation.org/packages/tensorflow/topics/export_savedmodel">export_savedmodel()</a></code> it’s straightforward to deploy it using TensorFlow Serving. See the documentation at <a href="https://www.tensorflow.org/serving" class="uri">https://www.tensorflow.org/serving</a> for additional details.</p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2 class="hasAnchor">
<a href="#sidebar" class="anchor"></a>License</h2>
<p>Apache License 2.0</p>
<h2>Developers</h2>
<ul class="list-unstyled">
<li>Javier Luraschi <br><small class="roles"> Author, maintainer </small> </li>
<li><a href="authors.html">All authors...</a></li>
</ul>
</div>

</div>


      <footer><div class="copyright">
  <p>Developed by Javier Luraschi.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
